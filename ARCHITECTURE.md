# Architecture Documentation# Architecture Documentation



## System Overview## System Overview



The Emotional RAG Backend is a production-ready FastAPI application that solves persona forgetting in LLM conversations through:The Emotional RAG Backend is a production-ready FastAPI application that solves persona forgetting in LLM conversations through proactive memory management, semantic retrieval, and emotional context tracking.

- **Proactive memory management** (3-tier system)

- **Semantic retrieval** (RAG with embeddings)---

- **Emotional context tracking** (importance scoring)

## Architecture Diagram

---

```

## High-Level Architectureâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚                        SillyTavern                          â”‚

```â”‚                  (Frontend / User Interface)                â”‚

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”‚                        SillyTavern                               â”‚                         â”‚ HTTP/REST

â”‚                   (Frontend / User Interface)                    â”‚                         â”‚ OpenAI-compatible API

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â†“

                           â”‚ HTTP/REST (OpenAI-compatible API)â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

                           â†“â”‚                     FastAPI Application                     â”‚

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚

â”‚                     FastAPI Application                          â”‚â”‚  â”‚              API Routes (app/routes/)                â”‚   â”‚

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚â”‚  â”‚  â€¢ /v1/chat/completions  (chat.py)                  â”‚   â”‚

â”‚  â”‚            API Routes (app/routes/)                        â”‚  â”‚â”‚  â”‚  â€¢ /v1/models            (chat.py)                  â”‚   â”‚

â”‚  â”‚  /v1/chat/completions  |  /v1/models  |  /health          â”‚  â”‚â”‚  â”‚  â€¢ /health               (health.py)                â”‚   â”‚

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚

â”‚               â”‚                        â”‚                         â”‚â”‚               â”‚                          â”‚                  â”‚

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚â”‚               â†“                          â†“                  â”‚

â”‚  â”‚   Core Services      â”‚   â”‚   Memory Manager               â”‚  â”‚â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚

â”‚  â”‚  (app/services/)     â”‚â—„â”€â–ºâ”‚   (app/core/memory.py)         â”‚  â”‚â”‚  â”‚   Core Services        â”‚  â”‚   Memory Manager       â”‚    â”‚

â”‚  â”‚                      â”‚   â”‚                                â”‚  â”‚â”‚  â”‚  (app/services/)       â”‚  â”‚  (app/core/memory.py)  â”‚    â”‚

â”‚  â”‚  â€¢ GeminiClient      â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚â”‚  â”‚                        â”‚  â”‚                        â”‚    â”‚

â”‚  â”‚  â€¢ RAGEngine         â”‚   â”‚  â”‚  Working Memory (RAM)   â”‚  â”‚  â”‚â”‚  â”‚  â€¢ GeminiClient        â”‚â†â†’â”‚  Working Memory (RAM)  â”‚    â”‚

â”‚  â”‚  â€¢ EmotionTracker    â”‚   â”‚  â”‚  deque[20 messages]     â”‚  â”‚  â”‚â”‚  â”‚  â€¢ RAGEngine           â”‚  â”‚  Short-Term (SQLite)   â”‚    â”‚

â”‚  â”‚  â€¢ TokenManager      â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚â”‚  â”‚  â€¢ EmotionTracker      â”‚  â”‚  Long-Term (Vectors)   â”‚    â”‚

â”‚  â”‚  â€¢ ChromaDBStore*    â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚â”‚  â”‚  â€¢ TokenManager        â”‚  â”‚                        â”‚    â”‚

â”‚  â”‚  â€¢ Reranker*         â”‚   â”‚  â”‚  Short-Term (SQLite)    â”‚  â”‚  â”‚â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚

â”‚  â”‚  â€¢ Transformer       â”‚   â”‚  â”‚  Full history + metadataâ”‚  â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”‚  â”‚    Emotions*         â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚                 â”‚                        â”‚

â”‚  â”‚  â€¢ RedisMemory*      â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚                 â†“                        â†“

â”‚  â”‚  â€¢ Metrics*          â”‚   â”‚  â”‚  Long-Term (Embeddings) â”‚  â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚  Semantic search (RAG)  â”‚  â”‚  â”‚â”‚    Google Gemini API    â”‚  â”‚   Data Persistence       â”‚

â”‚                              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚â”‚   (LLM Generation)      â”‚  â”‚                          â”‚

â”‚  * Phase 2 components        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚â”‚  â€¢ gemini-1.5-pro       â”‚  â”‚  â€¢ SQLite databases      â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â€¢ Streaming support    â”‚  â”‚    (per session)         â”‚

           â”‚                              â”‚â”‚  â€¢ Retry logic          â”‚  â”‚  â€¢ Embedding cache       â”‚

           â†“                              â†“â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â€¢ Conversation history  â”‚

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”‚  Google Gemini API     â”‚   â”‚    Data Persistence                â”‚```

â”‚  (LLM Generation)      â”‚   â”‚                                    â”‚

â”‚  â€¢ gemini-2.0-flash    â”‚   â”‚  â€¢ SQLite per session              â”‚---

â”‚  â€¢ Streaming support   â”‚   â”‚    (data/sessions/*.db)            â”‚

â”‚  â€¢ Retry logic         â”‚   â”‚  â€¢ ChromaDB vectors* (optional)    â”‚## Component Details

â”‚  â€¢ Rate limiting       â”‚   â”‚  â€¢ Redis cache* (optional)         â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â€¢ PostgreSQL* (optional)          â”‚### 1. API Layer (`app/routes/`)

                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```**chat.py** - Main chat endpoint

- OpenAI-compatible `/v1/chat/completions`

---- Streaming and non-streaming modes

- Session management via `user` field

## Component Details- Context building orchestration



### 1. API Layer (`app/routes/`)**health.py** - Health monitoring

- System status checks

#### chat.py - Main Chat Endpoint- Database connectivity

- Gemini API status

**Responsibilities**:- Active session count

- OpenAI-compatible `/v1/chat/completions`

- Streaming and non-streaming responses### 2. Core Services (`app/services/`)

- Session management via `user` field (SillyTavern sends chat_id here)

- Context building orchestration#### GeminiClient

```python

**Request Flow**:# Responsibilities:

```python- Async API calls to Google Gemini

1. Extract chat_id from request.user- Retry logic with exponential backoff

2. Detect emotion from user message- Rate limiting (5 concurrent requests)

3. Check/create persona from system message- Streaming response handling

4. Retrieve semantic context (RAG)- Token usage tracking

5. Build context with token budget```

6. Call Gemini API

7. Store messages with embeddings#### RAGEngine

8. Check summarization trigger```python

9. Return response# Responsibilities:

```- Text embedding with sentence-transformers

- Semantic similarity search

**Key Features**:- Persona chunking and indexing

- Uses SillyTavern's conversation context (from `request.messages`)- Context formatting

- Supplements with RAG-retrieved long-term memories- Embedding serialization

- Stores all messages for future retrieval```



#### health.py - Monitoring#### EmotionTracker

```python

**Endpoints**:# Responsibilities:

- `/health` - System status, DB connectivity, active sessions- Keyword-based emotion detection

- `/metrics` - Prometheus metrics (if `ENABLE_METRICS=true`)- Importance scoring (0-1 scale)

- Emotional context prompts

### 2. Core Services- Relevance boosting for matching emotions

```

#### GeminiClient (`app/services/gemini_client.py`)

#### TokenManager

**Purpose**: Async wrapper for Google Gemini API```python

# Responsibilities:

**Features**:- Token counting (tiktoken)

```python- Budget allocation across components

â€¢ Async API calls with asyncio.to_thread()- Message truncation

â€¢ Retry logic with exponential backoff (tenacity)- Context size optimization

â€¢ Rate limiting (max 5 concurrent requests)```

â€¢ Streaming response handling

â€¢ Token usage tracking### 3. Memory Manager (`app/core/memory.py`)

â€¢ Error handling for quota/rate limits

```The heart of the system - implements three-tier memory:



**Message Format Conversion**:#### Working Memory (RAM)

```python```python

# OpenAI format â†’ Gemini format# Structure: Dict[chat_id, deque(maxlen=20)]

[{role: "system", content: "..."},{

 {role: "user", content: "..."}]  "chat_123": deque([

    â†“    {"role": "user", "content": "...", "timestamp": "..."},

"INSTRUCTIONS:\n...\n\nUSER: ...\n\nASSISTANT:"    {"role": "assistant", "content": "...", "timestamp": "..."},

```    # ... last 20 messages

  ])

#### RAGEngine (`app/services/rag_engine.py`)}

```

**Purpose**: Semantic similarity search using sentence-transformers

#### Short-Term Memory (SQLite)

**Model**: `all-MiniLM-L6-v2` (384-dimensional embeddings)```sql

-- Per-session database: data/sessions/{chat_id}.db

**Capabilities**:

```python-- Full conversation history

â€¢ encode(text) â†’ np.ndarray[384]messages (

â€¢ search_embeddings(query, candidates, top_k)  id, chat_id, role, content,

â€¢ Emotional boosting (increases relevance for matching emotions)  embedding, emotional_state,

â€¢ Persona chunking (200 chars, 50 overlap)  importance_score, timestamp

â€¢ Embedding serialization (numpy â†” bytes))

```

-- Character definitions

**Search Algorithm**:personas (

```python  chat_id, persona_text,

1. Compute query embedding  embedding, updated_at

2. Calculate cosine similarity with all candidates)

3. Apply emotional boost: similarity *= (1 + importance * 0.3)

4. Sort by similarity descending-- Compressed history

5. Return top_k resultssummaries (

```  id, chat_id, summary_text,

  message_range, created_at

#### EmotionTracker (`app/services/emotion_tracker.py`))

```

**Purpose**: Keyword-based emotion detection + importance scoring

#### Long-Term Memory (Embeddings)

**Emotion Categories**:- Embeddings stored in SQLite BLOB format

- Joy, Sadness, Anger, Fear, Surprise, Disgust, Neutral- Semantic search via cosine similarity

- Emotional boosting for relevance

**Importance Scoring** (0.0 - 1.0):- Persona chunk indexing

```python

score = base_emotion_weight + length_factor---



# Factors:## Request Flow

â€¢ Emotion weight (joy: 0.7, anger: 0.8, etc.)

â€¢ Message length (longer = more important)### Standard Chat Completion

â€¢ Normalized to 0.0 - 1.0 range

``````

1. Request arrives at /v1/chat/completions

**Phase 2 Upgrade** (`transformer_emotions.py`):   â”œâ”€ Extract chat_id from request.user

- Uses `j-hartmann/emotion-english-distilroberta-base`   â””â”€ Parse user message

- 7 fine-grained emotions with confidence scores

- Multi-label support2. Emotion Detection

- 66% accuracy on test set   â”œâ”€ Analyze user message for emotion

   â”œâ”€ Calculate importance score

#### TokenManager (`app/core/token_manager.py`)   â””â”€ Store emotional state



**Purpose**: Token budget allocation and context building3. Memory Retrieval

   â”œâ”€ Check if persona exists

**Budget Allocation** (128k context window):   â”‚  â””â”€ If not, extract and index

```   â”œâ”€ Retrieve semantic context (RAG)

System/Persona:     20% â†’ ~25,600 tokens (persona NEVER truncated at start)   â”‚  â”œâ”€ Generate query embedding

RAG Context:        25% â†’ ~32,000 tokens (retrieved memories)   â”‚  â”œâ”€ Search message embeddings

History:            35% â†’ ~44,800 tokens (recent conversation)   â”‚  â””â”€ Boost emotionally similar

Response Buffer:    20% â†’ ~25,600 tokens (LLM output space)   â””â”€ Get recent conversation history

```

4. Context Building

**Key Methods**:   â”œâ”€ Allocate token budget

```python   â”‚  â”œâ”€ System/Persona: 20%

â€¢ allocate_token_budget() â†’ dict[str, int]   â”‚  â”œâ”€ RAG Context: 25%

â€¢ truncate_to_token_limit(text, max_tokens, preserve_start=True)   â”‚  â”œâ”€ History: 35%

â€¢ fit_messages_to_budget(messages, budget, keep_recent=10)   â”‚  â””â”€ Response: 20%

â€¢ count_tokens(text) â†’ int (using tiktoken cl100k_base)   â”œâ”€ Build system prompt

```   â”œâ”€ Format RAG results

   â”œâ”€ Fit history to budget

### 3. Memory Management   â””â”€ Assemble final context



#### MemoryManager (`app/core/memory.py`)5. LLM Generation

   â”œâ”€ Call Gemini API

**Purpose**: Orchestrates 3-tier memory system   â”‚  â”œâ”€ With retry logic

   â”‚  â””â”€ Rate limiting

**Architecture**:   â””â”€ Stream or return full response

```python

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”6. Storage

â”‚  Working Memory (RAM)               â”‚   â”œâ”€ Store user message

â”‚  deque[maxlen=20] per chat_id       â”‚   â”‚  â”œâ”€ Generate embedding

â”‚  â€¢ Instant access                   â”‚   â”‚  â”œâ”€ Save emotion metadata

â”‚  â€¢ Last 10-20 messages              â”‚   â”‚  â””â”€ Calculate importance

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”œâ”€ Store assistant message

           â†“ (writes)   â””â”€ Update working memory

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚  Short-Term Memory (SQLite)         â”‚7. Post-Processing

â”‚  data/sessions/{chat_id}.db         â”‚   â”œâ”€ Check if summarization needed

â”‚  â€¢ Full conversation history        â”‚   â”‚  â””â”€ If yes, trigger background task

â”‚  â€¢ Embeddings as BLOB               â”‚   â””â”€ Return response to client

â”‚  â€¢ Emotion + importance metadata    â”‚```

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

           â†“ (indexed)---

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚  Long-Term Memory (Vectors)         â”‚## Memory Management Strategy

â”‚  SQLite BLOB or ChromaDB*           â”‚

â”‚  â€¢ Semantic similarity search       â”‚### Token Budget Allocation

â”‚  â€¢ Emotional boosting               â”‚

â”‚  â€¢ Cross-encoder reranking*         â”‚For a 4096 token context window:

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

``````

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

**Database Schema**:â”‚  System/Persona (20%) = 800 tokens      â”‚ Always included

```sqlâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

-- messages tableâ”‚  RAG Context (25%) = 1000 tokens        â”‚ Semantic retrieval

CREATE TABLE messages (â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

    id INTEGER PRIMARY KEY,â”‚  History (35%) = 1400 tokens            â”‚ Recent conversation

    chat_id TEXT NOT NULL,â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

    role TEXT NOT NULL,â”‚  Response Buffer (20%) = 800 tokens     â”‚ Model output

    content TEXT NOT NULL,â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    embedding BLOB,              -- numpy array as bytes```

    emotional_state TEXT,         -- detected emotion

    importance_score REAL,        -- 0.0 - 1.0### Summarization Trigger

    timestamp DATETIME

);```python

# Automatic summarization when:

-- personas tableif messages_since_last_summary >= 20:

CREATE TABLE personas (    summary = await gemini.summarize(old_messages)

    chat_id TEXT PRIMARY KEY,    store_summary(summary)

    persona_text TEXT NOT NULL,    # Old messages compressed to ~200 word summary

    embedding BLOB,    # Preserves: emotional moments, decisions, context

    updated_at DATETIME```

);

### Context Building Algorithm

-- summaries table

CREATE TABLE summaries (```python

    id INTEGER PRIMARY KEY,def build_context(chat_id, user_message):

    chat_id TEXT NOT NULL,    """

    summary_text TEXT NOT NULL,    Smart context assembly with priority:

    message_range TEXT,          -- "1-20"    1. ALWAYS include full persona (never truncate)

    created_at DATETIME    2. Retrieve top-3 semantically relevant chunks

);    3. Add recent 10 messages (or fit to budget)

```    4. Include summaries if history truncated

    5. Add current user message

**Key Methods**:    """

```python    context = []

â€¢ store_message(chat_id, role, content, emotion, importance)    

â€¢ get_recent_messages(chat_id, limit=20)    # Priority 1: Persona (critical)

â€¢ retrieve_semantic_context(chat_id, query, emotion, top_k=3)    persona = get_persona(chat_id)

â€¢ store_persona(chat_id, persona_text, generate_embeddings=True)    context.append({"role": "system", "content": persona})

â€¢ should_summarize(chat_id) â†’ bool (checks message count)    

```    # Priority 2: RAG context (semantic relevance)

    rag = retrieve_semantic(user_message, top_k=3)

---    if rag:

        context.append({"role": "system", "content": rag})

## Phase 2 Enhancements    

    # Priority 3: History (recent conversation)

### ChromaDBVectorStore (`app/services/chromadb_store.py`)    history = get_recent_messages(chat_id, limit=20)

    fitted_history = fit_to_budget(history, budget=1400)

**Replaces**: SQLite BLOB storage for embeddings    

    if len(fitted_history) < len(history):

**Benefits**:        # History truncated, add summary

- Persistent HNSW indices (faster startup, no rebuild)        summary = get_latest_summary(chat_id)

- Native vector operations        context.append({"role": "system", "content": summary})

- Scalable to 100k+ embeddings    

- Metadata filtering    context.extend(fitted_history)

    

**Configuration**:    # Priority 4: Current message

```python    context.append({"role": "user", "content": user_message})

# Persistent mode (default)    

CHROMADB_PATH=./data/chromadb    return context

```

# Client-server mode

CHROMADB_HOST=localhost---

CHROMADB_PORT=8000

```## Semantic Retrieval (RAG)



### Reranker (`app/services/reranker.py`)### Embedding Generation



**Purpose**: Two-stage retrieval for better accuracy```python

# Model: all-MiniLM-L6-v2 (sentence-transformers)

**Model**: `cross-encoder/ms-marco-MiniLM-L6-v2` (MRR@10: 74.30)# Dimension: 384

# Speed: ~1000 sentences/sec on CPU

**Workflow**:

```pythontext = "I love pizza"

1. Bi-encoder retrieves top 50 candidates (fast, approximate)embedding = model.encode(text)

2. Cross-encoder reranks â†’ top 10 (slow, accurate)# â†’ numpy array [0.123, -0.456, 0.789, ...]

3. Result: Better relevance with minimal latency```

```

### Similarity Search

**Performance**:

- Throughput: ~1800 queries/second```python

- Latency: <1ms per query-document pair# Cosine similarity between query and candidates

query_emb = encode("What food do I like?")

### TransformerEmotionDetector (`app/services/transformer_emotions.py`)

scores = []

**Purpose**: Fine-grained emotion detection with confidencefor msg in messages_with_embeddings:

    similarity = cosine_similarity(query_emb, msg.embedding)

**Model**: `j-hartmann/emotion-english-distilroberta-base`    

    # Emotional boosting

**Features**:    if msg.emotion == query_emotion:

```python        similarity *= (1 + msg.importance_score * 0.3)

â€¢ 7 emotions: anger, disgust, fear, joy, neutral, sadness, surprise    

â€¢ Confidence scores (0.0 - 1.0)    scores.append((similarity, msg))

â€¢ Multi-label support (multiple emotions per message)

â€¢ Fallback to keyword-based if model fails# Return top-k

```return sorted(scores, reverse=True)[:3]

```

**Accuracy**: 66% on test set

### Persona Chunking

### RedisMemoryStore (`app/services/redis_memory.py`)

```python

**Purpose**: Distributed working memory across multiple workers# Large personas are chunked for better retrieval

persona = "You are Aria, a 25-year-old AI researcher..."  # 2000 chars

**Benefits**:

- Shared memory (no per-worker duplication)chunks = chunk_text(persona, chunk_size=200, overlap=50)

- TTL-based expiration (30 min default)# â†’ [

- Pub/sub for cache invalidation#     "You are Aria, a 25-year-old AI researcher...",

- Enables horizontal scaling#     "...researcher who loves science fiction and...",

#     "...fiction and philosophy. You have a warm...",

**Data Structure**:#     # etc.

```python# ]

# Sorted set per chat_id

redis_key = f"memory:{chat_id}"# Each chunk gets its own embedding

ZADD memory:chat_123 timestamp1 message1_jsonembeddings = encode_batch(chunks)

ZADD memory:chat_123 timestamp2 message2_json

# Stored in database for semantic search

# TTL```

EXPIRE memory:chat_123 1800  # 30 minutes

```---



### MetricsCollector (`app/services/metrics.py`)## Emotional Context System



**Purpose**: Prometheus observability### Emotion Detection



**Metrics**:```python

```python# Keywords-based (Phase 1)

# Countersemotions = {

chat_requests_total    'joy': ['happy', 'excited', 'love', 'ðŸ˜Š'],

chat_errors_total    'sadness': ['sad', 'hurt', 'cry', 'ðŸ˜¢'],

    'anger': ['angry', 'furious', 'ðŸ˜ '],

# Histograms    'fear': ['scared', 'worried', 'ðŸ˜°'],

chat_response_time_seconds    # etc.

token_usage_total}



# Gauges# Count matches

active_sessionsscores = {emotion: count_keywords(text, kws) 

memory_usage_bytes          for emotion, kws in emotions.items()}



# Summaries# Dominant emotion

rag_retrieval_latencyemotion = max(scores, key=scores.get)

emotion_detection_latencyconfidence = min(scores[emotion] / 3.0, 1.0)

``````



**Access**: `http://localhost:8001/metrics`### Importance Scoring



---```python

def calculate_importance(text, emotion, confidence):

## Request Processing Flow    score = 0.5  # baseline

    

### Chat Completion Request    # Factor 1: Emotional intensity

    if emotion != 'neutral':

```        score += confidence * emotion_weight * 0.3

1. SillyTavern Request Arrives    

   â†“    # Factor 2: Length (detail indicator)

   POST /v1/chat/completions    if len(text) > 200:

   {        score += 0.15

     "messages": [...full conversation...],    

     "user": "chat_session_123",  â† chat_id    # Factor 3: Questions (engagement)

     "model": "gemini-2.0-flash-exp"    score += min(text.count('?') * 0.1, 0.15)

   }    

    # Factor 4: Exclamations (emphasis)

2. Extract Context    score += min(text.count('!') * 0.05, 0.1)

   â†“    

   chat_id = request.user || "default"    # Factor 5: Personal pronouns (investment)

   user_message = last message with role="user"    pronouns = ['i ', 'me ', 'my ']

   history = all previous user/assistant messages    score += min(count_pronouns(text) * 0.05, 0.1)

    

3. Emotion Detection    return min(score, 1.0)

   â†“```

   emotion_state = emotion_tracker.detect_emotion(user_message)

   # Returns: {emotion: "joy", confidence: 0.85, importance: 0.7}### Dynamic System Prompts



4. Persona Retrieval/Storage```python

   â†“# Emotional context added to system prompt

   persona = await memory_manager.get_persona(chat_id)if emotion == 'sadness' and confidence > 0.6:

   if not persona:    prompt += """

       extract from system message in request.messages    ## Emotional Context

       store with embeddings    User is feeling sad (confidence: 85%)

    Respond with empathy, validation, and gentle support.

5. RAG Retrieval (Semantic Search)    

   â†“    **Relevant past emotional moments:**

   query_embedding = rag_engine.encode(user_message)    - "I was really hurt when..." (3 days ago)

       - "Feeling down about..." (1 week ago)

   # Search database    """

   relevant_messages = await memory_manager.retrieve_semantic_context(```

       chat_id, 

       user_message, ---

       emotion=emotion_state.emotion,

       top_k=3## Performance Characteristics

   )

   ### Latency Breakdown

   # Phase 2: Rerank with cross-encoder

   if ENABLE_RERANKING:```

       relevant_messages = reranker.rerank(user_message, relevant_messages)Total Response Time: ~1.5-3.0 seconds



6. Context Buildingâ”œâ”€ Emotion Detection:     10-20ms

   â†“â”œâ”€ RAG Retrieval:         50-100ms

   budget = token_manager.allocate_token_budget()â”‚  â”œâ”€ Embedding generation: 20ms

   â”‚  â”œâ”€ Similarity search:    30ms

   context_messages = [â”‚  â””â”€ Formatting:           10ms

       {role: "system", content: persona},           # 20% budgetâ”œâ”€ Context Building:      20-50ms

       {role: "system", content: rag_context},       # 25% budgetâ”œâ”€ Gemini API Call:       1000-2500ms

       ...history_messages,                          # 35% budgetâ”‚  â”œâ”€ Network latency:    100-200ms

       {role: "user", content: user_message}â”‚  â””â”€ Model generation:   900-2300ms

   ]â””â”€ Storage:               50-100ms

   â”œâ”€ Embedding:          30ms

7. Gemini API Call   â””â”€ Database write:     20ms

   â†“```

   response = await gemini_client.chat_completion(

       messages=context_messages,### Resource Usage

       temperature=0.9,

       max_tokens=800```

   )Memory:

â”œâ”€ Base application:     ~200MB

8. Store Messagesâ”œâ”€ Embedding model:      ~400MB

   â†“â”œâ”€ Working memory:       ~50MB per 100 sessions

   await memory_manager.store_message(â””â”€ Total (typical):      ~650MB

       chat_id, "user", user_message, 

       emotion=emotion_state.emotion,Disk:

       importance=emotion_state.importance,â”œâ”€ Application code:     ~5MB

       generate_embedding=Trueâ”œâ”€ Dependencies:         ~800MB

   )â”œâ”€ Embedding model:      ~80MB

   â”œâ”€ Per-session DB:       ~100KB-5MB

   await memory_manager.store_message(â””â”€ Total (typical):      ~1GB

       chat_id, "assistant", response.content,

       generate_embedding=TrueCPU:

   )â”œâ”€ Idle:                 <1%

â”œâ”€ During request:       20-40% (embedding)

9. Check Summarizationâ””â”€ Concurrent requests:  Scales linearly

   â†“```

   if await memory_manager.should_summarize(chat_id):

       asyncio.create_task(### Scalability

           memory_manager.summarize_conversation(chat_id)

       )```

Single Instance:

10. Return Responseâ”œâ”€ Concurrent requests:  5 (Gemini rate limit)

    â†“â”œâ”€ Sessions:            1000+ (limited by RAM)

    return ChatCompletionResponse(...)â”œâ”€ Messages/day:        10,000+

```â””â”€ Disk growth:         ~10MB/1000 messages



---Multi-Instance (Phase 2):

â”œâ”€ Add Redis for shared working memory

## Data Flow Diagramsâ”œâ”€ Load balancer for API requests

â”œâ”€ Shared PostgreSQL for persistence

### Memory Storage Flowâ””â”€ Handles: 100+ concurrent users

```

```

User Message---

    â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”## Security Considerations

â”‚ 1. Add to Working   â”‚

â”‚    Memory (RAM)     â”‚### Current Implementation

â”‚    deque.append()   â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜```

           â†“âœ… Environment-based secrets (.env)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”âœ… CORS middleware (configurable origins)

â”‚ 2. Generate         â”‚âœ… Input validation (Pydantic models)

â”‚    Embedding        â”‚âœ… SQL injection protection (parameterized queries)

â”‚    RAG.encode()     â”‚âœ… Rate limiting (per Gemini API)

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

           â†“âš ï¸  No authentication (local use assumed)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”âš ï¸  HTTP only (no TLS)

â”‚ 3. Store in SQLite  â”‚âš ï¸  Open CORS (development mode)

â”‚    with metadata    â”‚```

â”‚    (emotion,        â”‚

â”‚     importance)     â”‚### Production Recommendations

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

           â†“```python

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”# 1. Add authentication

â”‚ 4. Optional: Store  â”‚from fastapi import Depends, HTTPException

â”‚    in ChromaDB*     â”‚from fastapi.security import HTTPBearer

â”‚    (Phase 2)        â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜security = HTTPBearer()

```

@app.post("/v1/chat/completions")

### RAG Retrieval Flowasync def chat(request: Request, token: str = Depends(security)):

    verify_token(token)  # Implement token validation

```    # ...

Query: "What is my name?"

    â†“# 2. Enable HTTPS

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”uvicorn.run(

â”‚ 1. Detect Emotion        â”‚    app,

â”‚    emotion = "neutral"   â”‚    ssl_keyfile="key.pem",

â”‚    confidence = 0.5      â”‚    ssl_certfile="cert.pem"

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜)

             â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”# 3. Restrict CORS

â”‚ 2. Generate Embedding    â”‚app.add_middleware(

â”‚    query_emb = [0.2,     â”‚    CORSMiddleware,

â”‚    0.5, -0.1, ...]       â”‚    allow_origins=["https://your-sillytavern.com"],

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    allow_credentials=True,

             â†“    allow_methods=["POST", "GET"],

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    allow_headers=["Content-Type", "Authorization"]

â”‚ 3. Retrieve Candidates   â”‚)

â”‚    (Bi-encoder)          â”‚

â”‚    Top 50 by cosine      â”‚# 4. Rate limiting per user

â”‚    similarity            â”‚from slowapi import Limiter

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜limiter = Limiter(key_func=get_remote_address)

             â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”@app.post("/v1/chat/completions")

â”‚ 4. Apply Emotional Boost â”‚@limiter.limit("60/minute")

â”‚    if msg.emotion ==     â”‚async def chat(...):

â”‚       query.emotion:     â”‚    # ...

â”‚      score *= (1 + imp)  â”‚```

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

             â†“---

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚ 5. Rerank* (Phase 2)     â”‚## Monitoring & Observability

â”‚    Cross-encoder:        â”‚

â”‚    50 â†’ 10 best          â”‚### Structured Logging

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

             â†“```json

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”{

â”‚ 6. Format for Context    â”‚  "timestamp": "2025-11-04T10:30:45.123Z",

â”‚    "Retrieved Context:   â”‚  "level": "INFO",

â”‚     - My name is Alice   â”‚  "name": "app.routes.chat",

â”‚     - I love coding"     â”‚  "message": "Chat completion successful",

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  "chat_id": "user_123",

```  "input_tokens": 1234,

  "output_tokens": 456,

---  "total_tokens": 1690,

  "latency_seconds": 2.34

## Configuration}

```

### Environment Variables

### Health Metrics

See `.env.example` for full configuration.

```python

**Core Settings**:GET /health

```env

GEMINI_API_KEY=...{

GEMINI_MODEL=gemini-2.0-flash-exp  "status": "healthy",

PORT=8001  "gemini_api": true,

```  "database": true,

  "memory_sessions": 12,

**Memory**:  "timestamp": "2025-11-04T10:30:45Z"

```env}

MAX_WORKING_MEMORY_SIZE=20```

SUMMARIZE_AFTER_MESSAGES=20

DB_PATH=./data/sessions### Usage Tracking

```

```python

**RAG**:# Per-session cost tracking

```envsession_stats = {

EMBEDDING_MODEL=all-MiniLM-L6-v2    "total_input_tokens": 45000,

RAG_TOP_K=3    "total_output_tokens": 12000,

```    "estimated_cost": 0.12  # USD

}

**Token Budget**:```

```env

SYSTEM_TOKEN_PERCENT=20---

RAG_TOKEN_PERCENT=25

HISTORY_TOKEN_PERCENT=35## Deployment Options

RESPONSE_TOKEN_PERCENT=20

```### Option 1: Local Development

```bash

**Phase 2 Features**:uvicorn app.main:app --reload --host localhost --port 8000

```env```

ENABLE_CHROMADB=true

ENABLE_RERANKING=true### Option 2: Production Server

ENABLE_TRANSFORMER_EMOTIONS=true```bash

ENABLE_REDIS=false          # Needs Redis servergunicorn app.main:app \

ENABLE_POSTGRESQL=false     # Needs PostgreSQL server  --workers 4 \

ENABLE_METRICS=true  --worker-class uvicorn.workers.UvicornWorker \

```  --bind 0.0.0.0:8000

```

---

### Option 3: Docker (Phase 2)

## Deployment Architecture```dockerfile

FROM python:3.11-slim

### Single Server (Default)WORKDIR /app

COPY requirements.txt .

```RUN pip install -r requirements.txt

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”COPY . .

â”‚  Server (localhost:8001)    â”‚CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0"]

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚```

â”‚  â”‚  FastAPI + Uvicorn    â”‚  â”‚

â”‚  â”‚  (single worker)      â”‚  â”‚### Option 4: Cloud (AWS/GCP/Azure)

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚```yaml

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚# Example: AWS Elastic Beanstalk

â”‚  â”‚  SQLite per session   â”‚  â”‚environment:

â”‚  â”‚  data/sessions/*.db   â”‚  â”‚  - GEMINI_API_KEY: ${GEMINI_KEY}

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  - DB_PATH: /data/sessions

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜scaling:

```  min_instances: 1

  max_instances: 5

### Production (Docker Compose)```



```---

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚  Load Balancer   â”‚â”€â”€â”€â–¶â”‚  API (x4)        â”‚## Future Enhancements (Phase 2)

â”‚  (nginx)         â”‚    â”‚  Gunicorn        â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  + Uvicorn       â”‚### Advanced RAG

                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- ChromaDB for persistent vector storage

                                 â”‚- Cross-encoder reranking

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”- Hybrid search (semantic + keyword)

        â†“                        â†“                 â†“- Metadata filtering

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚  ChromaDB    â”‚      â”‚  Redis       â”‚  â”‚  PostgreSQL  â”‚### Emotion Detection

â”‚  (vectors)   â”‚      â”‚  (cache)     â”‚  â”‚  (history)   â”‚- Transformer-based models (BERT)

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- Multi-label classification

        â†“                        â†“                 â†“- Sentiment intensity scoring

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚              Monitoring (Prometheus + Grafana)       â”‚### Distributed System

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- Redis for shared memory

```- PostgreSQL for persistence

- Message queue (RabbitMQ)

---- Horizontal scaling



## Performance Characteristics### Monitoring

- Prometheus metrics

### Latency (Typical)- Grafana dashboards

- OpenTelemetry tracing

```- Alert system

Component              | Latency

-----------------------|-------------

Emotion Detection      | <5ms

Embedding Generation   | 10-50ms (per message)## Troubleshooting Guide

RAG Retrieval          | 10-100ms (depends on DB size)

Cross-Encoder Rerank   | 5-20ms (for 50â†’10)### Common Issues

Gemini API Call        | 1-3s (network + generation)

Database Write         | 5-10ms**High Memory Usage**

Total (non-streaming)  | 1.5-4s```bash

```# Solution: Reduce working memory size

# In .env:

### ThroughputMAX_WORKING_MEMORY_SIZE=10

```

```

Concurrent Requests: 5 (rate limited at Gemini client)**Slow Responses**

Requests/min: ~100 (limited by Gemini API)```bash

Messages stored/sec: 200+# Solution: Reduce RAG top-k

RAG searches/sec: 100+RAG_TOP_K=1

```

# Or use faster model

### StorageGEMINI_MODEL=gemini-1.5-flash

```

```

SQLite per session: ~1-10 MB (depends on conversation length)**Database Locked**

Embedding storage: ~1.5 KB per message (384 floats Ã— 4 bytes)```bash

ChromaDB index: ~2x raw embedding size# Solution: Close stale connections

```sqlite3 data/sessions/chat.db "PRAGMA journal_mode=WAL;"

```

---

**Embedding Errors**

## Security Considerations```bash

# Solution: Clear model cache

1. **API Key Management**rm -rf ~/.cache/torch/sentence_transformers/

   - Store in `.env` (not in code)```

   - Never commit `.env` to git

   - Use environment-specific keys---



2. **Session Isolation**## Performance Tuning

   - Each chat_id gets separate SQLite DB

   - No cross-session data leakage### For Speed

   - Sessions auto-cleanup via TTL (if Redis enabled)```env

EMBEDDING_MODEL=all-MiniLM-L6-v2  # Fastest

3. **Input Validation**RAG_TOP_K=1

   - Pydantic models validate all inputsMAX_WORKING_MEMORY_SIZE=10

   - SQL injection prevented (parameterized queries)SUMMARIZE_AFTER_MESSAGES=30

   - Token limits prevent resource exhaustion```



4. **Rate Limiting**### For Quality

   - Gemini client: max 5 concurrent```env

   - Can add API-level rate limiting with FastAPI middlewareEMBEDDING_MODEL=all-mpnet-base-v2  # Better quality

RAG_TOP_K=5

---MAX_WORKING_MEMORY_SIZE=30

SUMMARIZE_AFTER_MESSAGES=15

## Troubleshooting```



### Common Issues### For Cost Optimization

```env

**High Memory Usage**:GEMINI_MODEL=gemini-1.5-flash  # Cheaper

- Reduce `MAX_WORKING_MEMORY_SIZE`SYSTEM_TOKEN_PERCENT=15

- Enable Redis for distributed memoryRAG_TOKEN_PERCENT=20

- Clear old session databases```



**Slow RAG Retrieval**:---

- Enable `ENABLE_CHROMADB` for HNSW indices

- Reduce `RAG_TOP_K`**This architecture is designed to be production-ready, scalable, and easy to extend!**

- Enable `ENABLE_RERANKING` for better accuracy with fewer candidates

**Database Corruption**:
```bash
# Backup and delete
mv data/sessions/*.db data/sessions/backup/
# Restart server
```

**Gemini Rate Limits**:
- Upgrade to paid tier
- Reduce concurrent requests
- Add exponential backoff (already implemented)

---

## Future Enhancements

### Planned Features

1. **Multi-LLM Support**
   - OpenAI, Anthropic, local models
   - Fallback chains

2. **Advanced Summarization**
   - Hierarchical summaries
   - Key moment extraction
   - Relationship graphs

3. **Memory Compression**
   - Adaptive context windows
   - Importance-based pruning
   - Smart summarization triggers

4. **Multi-modal Embeddings**
   - Image understanding
   - Voice input
   - Document processing

---

For implementation details, see the source code in `app/`. For usage instructions, see **QUICKSTART.md**.
